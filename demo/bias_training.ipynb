{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Detection Model Fine-tuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load one of the .xlsx files\n",
    "df = pd.read_csv('./data/MBIC/final_labels_MBIC.csv', sep=';', encoding='utf-8')\n",
    "df.drop(columns=['type', 'topic', 'outlet', 'news_link'], inplace=True)\n",
    "\n",
    "df.dropna(inplace=True)  # Remove missing values\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# clean the \n",
    "# remove no agreement rows\n",
    "df = df[df['label_bias'] != 'No agreement']\n",
    "\n",
    "# Convert categorical labels to numerical labels\n",
    "le = LabelEncoder()\n",
    "le.fit(['Non-biased', 'Biased'])\n",
    "df['label_bias_numeric'] = le.transform(df['label_bias'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: the label encoder marks Biased as 0, Non-biased as 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13343\\anaconda3\\envs\\Dbias\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load trained model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-MBIC-2-tokenizer\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilroberta-MBIC-2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the classification function\n",
    "def classify_text(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    ## Move the tokenized inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get the predicted label\n",
    "    predicted_label = outputs.logits.argmax().item()\n",
    "    \n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [item['input_ids'].squeeze(0) for item in batch]  # Remove the extra dimension\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)  # Set batch_first to True\n",
    "    \n",
    "    return {'input_ids': texts_padded, 'labels': labels}\n",
    "\n",
    "\n",
    "# 1. Split the dataframe into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Preprocess the text data and convert it into numerical features\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = df['label_bias_numeric'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {key: tensor.squeeze(0) for key, tensor in inputs.items()}  # Remove the extra batch dimension\n",
    "        inputs['labels'] = torch.tensor(label)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(train_df)\n",
    "val_dataset = TextDataset(val_df)\n",
    "\n",
    "# 3. Define the training loop and optimization process\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Calculate class weights for imbalanced distribution\n",
    "class_counts = train_df['label_bias_numeric'].value_counts().to_list()\n",
    "class_weights = [sum(class_counts) / c for c in class_counts]\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Use the weights in the loss function\n",
    "# This can help the model pay more attention to underrepresented classes, potentially improving recall.\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 4. Train the model on the training set\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clipping\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    # 5. Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predicted_labels = outputs.logits.argmax(dim=1)\n",
    "            val_correct += (predicted_labels == inputs['labels']).sum().item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_accuracy = val_correct / len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation on unseen data (to avoid the similar language style from same data source)\n",
    "sample_data = pd.read_csv('./data/generated_data.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# le.fit(['Non-biased', 'Biased'])\n",
    "sample_data['label_bias_numeric'] = le.transform(sample_data['Label'])\n",
    "\n",
    "# Create a new column 'prediction' in df\n",
    "sample_data['prediction'] = 0\n",
    "\n",
    "# Apply the classify_text_apply function on each row of 'Text' column with tqdm progress bar\n",
    "sample_data['prediction'] = sample_data['Text'].progress_apply(classify_text)\n",
    "\n",
    "accuracy = accuracy_score(sample_data['label_bias_numeric'], sample_data['prediction'])\n",
    "f1 = f1_score(sample_data['label_bias_numeric'], sample_data['prediction'])\n",
    "precision = precision_score(sample_data['label_bias_numeric'], sample_data['prediction'])\n",
    "recall = recall_score(sample_data['label_bias_numeric'], sample_data['prediction'])\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dbias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
